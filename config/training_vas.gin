import sdk.Model.DDPG
import sdk.Model.USCB
import sdk.Model.CalculateOptimalR
import sdk.Model.BackGroundActors
import sdk.Agent.Agents
import sdk.Environment.RealAdvertisingSystem
import sdk.Environment.VirtualAdvertisingSystem
import sdk.Common.Utils
import run.run_ras
import run.run_vas
import gin

RANDOM_SEED = 5

ALGORITHM = "DDPG"
GPU = '4'

EPISODE = 10000000
COL_EPISODE = 1
STEP = 96
BUFFER_SIZE = 100000
SAMPLE_SIZE = 20000
NUM_AGENT = 100
DIM_OBS = 3
DIM_ACTIONS = 1
TRAIN_MODE = "train_vas"
TEST_MODE = "test"
REPRESENT_INDEX = 0
STORE_RANKING_LOG = False
CRITIC_LR = 0.0001
ACTOR_LR = 0.0001
RANKING_LOG_EPI = 1
TAU = 0.1
GAMMA = 1
EVALUATION_INTERVAL = 5
TERMINAL_BUDGET_THRESHOLD = 5
UPDATE_TARGET_EPI = 5
PRE_TRAIN_Q_ITERATION = 10000
PRE_TRAIN_Q_FLAG = False
Q_TRAIN_ITE = 2
ACTOR_TRAIN_ITE = 1
FIXED_RANDOM_SEED = 0

MAX_BUDGET = 4000
MIN_BUDGET = 1000
MAX_PV_NUM = 500
MIN_PV_NUM = 100
REPRESENT_BUDGET = 1500

RealAdvertisingSystem.num_of_campaign = %NUM_AGENT
RealAdvertisingSystem.reserve_price = 0.01
RealAdvertisingSystem.T = %STEP
RealAdvertisingSystem.fixed_random_seed = %FIXED_RANDOM_SEED
RealAdvertisingSystem.min_pv_num = %MIN_PV_NUM
RealAdvertisingSystem.max_pv_num = %MAX_PV_NUM
RealAdvertisingSystem.terminate_threshold = %TERMINAL_BUDGET_THRESHOLD

VirtualAdvertisingSystem.num_campaign = %NUM_AGENT
VirtualAdvertisingSystem.num_epi = %RANKING_LOG_EPI
VirtualAdvertisingSystem.num_step = %STEP
VirtualAdvertisingSystem.log_name = "virtual_advertising_stage_2_log"
VirtualAdvertisingSystem.terminate_threshold = %TERMINAL_BUDGET_THRESHOLD

random_sampling.sample_number = %SAMPLE_SIZE
random_sampling.buffer_size = %BUFFER_SIZE

normalize_state.num_step = %STEP
normalize_state.max_budget = %MAX_BUDGET

normalize_state_batch.num_step = %STEP
normalize_state_batch.max_budget = %MAX_BUDGET

Agents.num_agent = %NUM_AGENT
Agents.algorithm_name = %ALGORITHM
Agents.bgactors = @BGActors
Agents.representation_index = %REPRESENT_INDEX
Agents.dim_obs = %DIM_OBS
Agents.stop_threshold = %TERMINAL_BUDGET_THRESHOLD


DDPG.Critic.dim_observation = %DIM_OBS
DDPG.Critic.dim_action = %DIM_ACTIONS

DDPG.Actor.dim_observation = %DIM_OBS
DDPG.Actor.dim_action = %DIM_ACTIONS

DDPG.dim_obs = %DIM_OBS
DDPG.dim_actions = %DIM_ACTIONS
DDPG.gamma = %GAMMA
DDPG.tau = %TAU
DDPG.critic_lr = %CRITIC_LR
DDPG.actor_lr = %ACTOR_LR
DDPG.buffer_size = %BUFFER_SIZE
DDPG.sample_size = %SAMPLE_SIZE
DDPG.network_random_seed = %RANDOM_SEED
DDPG.Q_train_ite = %Q_TRAIN_ITE
DDPG.Actor_train_ite = %ACTOR_TRAIN_ITE

BackGroundActors.Actor.dim_observation = %DIM_OBS
BackGroundActors.Actor.dim_action = %DIM_ACTIONS

BGActors.dim_obs = %DIM_OBS
BGActors.dim_actions = %DIM_ACTIONS
BGActors.fixed_random_seed = %FIXED_RANDOM_SEED

USCB.Critic.dim_observation = %DIM_OBS
USCB.Critic.dim_action = %DIM_ACTIONS

USCB.Actor.dim_observation = %DIM_OBS
USCB.Actor.dim_action = %DIM_ACTIONS

USCB.dim_obs = %DIM_OBS
USCB.dim_actions = %DIM_ACTIONS
USCB.gamma = 1
USCB.tau = %TAU
USCB.critic_lr = %CRITIC_LR
USCB.actor_lr = %ACTOR_LR
USCB.buffer_size = %BUFFER_SIZE
USCB.sample_size = %SAMPLE_SIZE


run_vas.episode = %EPISODE
run_vas.collecting_data_episode = %COL_EPISODE
run_vas.len_step = %STEP
run_vas.num_agent = %NUM_AGENT
run_vas.dim_obs = %DIM_OBS
run_vas.test_mode = %TEST_MODE
run_vas.train_mode = %TRAIN_MODE
run_vas.target_update_epi = %UPDATE_TARGET_EPI
run_vas.saved_trained_net = True
run_vas.save_path="saved_models/VAS"
run_vas.test_flag = True
run_vas.min_budget = %MIN_BUDGET
run_vas.max_budget = %MAX_BUDGET
run_vas.store_ranking_log = %STORE_RANKING_LOG
run_vas.fixed_random_seed = %FIXED_RANDOM_SEED
run_vas.network_random_seed = %RANDOM_SEED
run_vas.evaluation_interval = %EVALUATION_INTERVAL
run_vas.gpu = %GPU
run_vas.representation_index = %REPRESENT_INDEX
run_vas.representation_budget = %REPRESENT_BUDGET
run_vas.pre_train_Q_iteration = %PRE_TRAIN_Q_ITERATION
run_vas.pre_train_Q_flag = %PRE_TRAIN_Q_FLAG
run_vas.algorithm_name = %ALGORITHM